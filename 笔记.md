# 说明

## 子目标

### 基于示性函数的终局奖励

终局事件 $E$ 的谓词 $p_E$ ，满足 $p_E(t,s)=1\implies f(t,s)=0$

$$
r(t,s,a,s'):=1\{p_E(t,s')\}
$$

诱导的价值函数

$$
\begin{aligned}
V_\pi(s)&=\mathbb{E}[\gamma^T p(T,s_T)|S_0=s]\\
&=\sum_{t=1}^\infty\gamma^t \Pr\{T=t,p_E(S_t)=1|S_0=s\}
\end{aligned}
$$

### 基于势函数的奖励函数

$$
r(s,a,s'):=\phi(s)-\gamma\phi(s')
$$

诱导的价值函数

$$
V_\pi(s)=\phi(s)-E[\gamma^T \phi(S_T)|S_0=s]
$$

### 基于MPC的二次型奖励

$$
\begin{matrix}
r(s,a,s'):=-\frac{1}{2}\|u(a)-u_c(s,s')\|_2^2
\end{matrix}
$$

其中 $u$ 是从动作解算得到角速度的算子， $u_c$ 是参考角速度，这里是由 $s'$ 直接得到的偏航角误差基于比例导引法计算（显然有至少1步的延迟）。

## 价值函数界估计

假设每种奖励函数都一致有界，即 $L_R^i=\inf_{\omega,t}R_t^i>-\infty, H_R^i=\sup_{\omega,t}R_t^i<\infty ,\forall i.$ 。那么可以估计（非终止状态集上的）价值函数上界

$$
\begin{aligned}
V^i(S_0)=\mathbb{E}\left[\sum_{t=1}^T \gamma^{t-1} R_t^i\middle|S_0\right]%
\leq \mathbb{E}\left[\sum_{t=1}^T \gamma^{t-1} H_R^i\middle|S_0\right]%
= H_R^i\mathbb{E}\left[\frac{1-\gamma^T}{1-\gamma} \middle|S_0\right]
\end{aligned}
$$

若 $H_R^i\geq 0$ ，则 $V^i(S_0)\leq \frac{1}{1-\gamma}H_R^i$ ；
若 $H_R^i<0$ ，则 $V^i(S_0)\leq H_R^i$ ；
同理估计价值函数下界
若 $L_R^i\geq 0$ ，则 $V^i(S_0)\geq L_R^i$ ；
若 $L_R^i<0$ ，则 $V^i(S_0)\geq \frac{1}{1-\gamma}L_R^i$ ；
简记 $V^i$ 的任一组上、下界分别为 $H_V^i,L_V^i$ 。

- 如果奖励函数是终局奖励（稀疏但绝对值大）
- 如果是有限时域问题

问题：加权后的奖励函数，其对应的价值函数界用哪种方式估计更紧？

由Bellman方程， $V^i(t,s)-b=\mathbb{E}[R_{t+1}^i-(1-\gamma)b+\gamma [V^i(t+1,S_{t+1})-b]|S_t=s]$ ，则 $\forall b\leq L_V^i$ ，有

$$
\begin{aligned}
0\leq\mathbb{E}\left[\sum_{t=1}^T \gamma^{t-1} [R_{t}^i-(1-\gamma)b]\middle|S_0=s\right]\geq H_V^i-b
\end{aligned}
$$

得到归一化方法。

实际上，即使用奖励函数界估计价值函数界然后归一化，在采样过程中依然会出现（无折扣）累计回报值在归一化后为绝对值接近零的负数，这非常诡异。
